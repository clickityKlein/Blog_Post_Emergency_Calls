{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1820fe4d",
   "metadata": {},
   "source": [
    "## Blog Post Functions\n",
    "This file contains functions created for use in the blog post project.\n",
    "\n",
    "Functions:\n",
    "1. vals_by_col\n",
    "2. date_to_col\n",
    "3. secondary_unique\n",
    "4. counts_to_portions\n",
    "5. expand_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d7d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libaries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b91fd6c",
   "metadata": {},
   "source": [
    "## Function 1: vals_by_col(df, cols)\n",
    "This function returns all unique values for specified columns in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba22f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vals_by_col(df, cols):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    - df: DataFrame containing columns to break into unique values\n",
    "    \n",
    "    - cols: list of columns in df in which unique values are desired \n",
    "    \n",
    "    OUTPUT:\n",
    "    - df_vals: Series with columns in cols as index and uniques as the values\n",
    "    \"\"\"\n",
    "    df_vals = {col: None for col in cols}\n",
    "    for col in cols:\n",
    "        df_vals[col] = list(df[col].value_counts().index)\n",
    "        \n",
    "    df_vals = pd.Series(df_vals)\n",
    "    return df_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710bfa12",
   "metadata": {},
   "source": [
    "## Function 2: date_to_col(df, col)\n",
    "This function splits a single entry containing information about date and time, returning a DataFrame with the information split into individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fd0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_col(df, col):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    - df: DataFrame containing columns with dates of type:\n",
    "    \"mm/dd/yyyy hh:mm:ss xx\", where xx is either \"AM\" or \"PM\"\n",
    "\n",
    "    - col: name of column which contains data of this type to be broken up  \n",
    "    \n",
    "    OUTPUT:\n",
    "    - new_df: will return df with original column removed, and new columns for \n",
    "    respective time splits\n",
    "    \"\"\"\n",
    "    month = []\n",
    "    day = []\n",
    "    year = []\n",
    "    hour = []\n",
    "    minute = []\n",
    "    second = []\n",
    "    for entry in df[col]:\n",
    "        month.append(int(entry[0:2]))\n",
    "        day.append(int(entry[3:5]))\n",
    "        year.append(int(entry[6:10]))\n",
    "        if entry[-2:] == 'AM':\n",
    "            hour.append(int(entry[11:13]))\n",
    "        else:\n",
    "            hour.append(int(entry[11:13])+12)\n",
    "        minute.append(int(entry[14:16]))\n",
    "        second.append(int(entry[17:19]))\n",
    "    \n",
    "    # drop original column and add new columns\n",
    "    new_df = df.drop(col, axis=1)\n",
    "    \n",
    "    cols = {'month': month, 'day': day, 'year': year,\n",
    "            'hour': hour, 'minute': minute, 'second': second}\n",
    "    for col_name in cols:\n",
    "        new_df[col+'_'+col_name] = cols[col_name]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac83494",
   "metadata": {},
   "source": [
    "## Function 3: secondary_unique(df, uniq_col, list_col, zero_missing)\n",
    "This function returns either a list of dictionaries or a DataFrame which contains the number of times each unique in a certain column intersects with values from a secondary column (associated appearances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf56e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondary_unique(df, uniq_col, list_col, zero_missing = 'no'):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    - df: dataframe which contains column to be separated into unique values, and\n",
    "    to choose secondary values to list\n",
    "    - uniq_col: column to be separated into unique values (main describer)\n",
    "    - list_col: column to make list of when a unique value from first list is found,\n",
    "    (i.e the secondary describer to be counted)\n",
    "    - zero_missing: boolean 'yes' or 'no', will add any missing overall secondaries with\n",
    "    the value of 0\n",
    "    \n",
    "    OUTPUT:\n",
    "    - listed_uniqs:\n",
    "        - if zero_missing == 'no': returns list of dictionaries for number of times\n",
    "        associated values appear\n",
    "        - if zero_missing == 'yes': returns dataframe of associated appearences\n",
    "    \"\"\"\n",
    "    \n",
    "    listed_uniqs = []\n",
    "    uniq = vals_by_col(df, [uniq_col])\n",
    "    for uniq_value in range(0, len(uniq[0])):\n",
    "        listed_series = df[df[uniq_col]==uniq[0][uniq_value]][list_col].value_counts()\n",
    "        listed_uniqs.append([uniq[0][uniq_value],dict(listed_series)])\n",
    "        \n",
    "    if zero_missing == 'yes':\n",
    "        secondary_list = df[list_col].value_counts().index\n",
    "        for secondary_counts in listed_uniqs:\n",
    "            for secondary_category in secondary_list:\n",
    "                if secondary_category not in secondary_counts[1]:\n",
    "                    secondary_counts[1][secondary_category] = 0\n",
    "                    \n",
    "        all_data = []\n",
    "        for secondary in listed_uniqs:\n",
    "            all_data.append(pd.Series(secondary[1], name = secondary[0]))\n",
    "            \n",
    "        listed_uniqs = pd.DataFrame(all_data).transpose()\n",
    "        \n",
    "    return listed_uniqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115d7e1",
   "metadata": {},
   "source": [
    "## Function 4: counts_to_portions(df, variant)\n",
    "Given a DataFrame with counts as the datapoints, this function will return a DataFrame with the datapoints turned into a proportion, calculated by:\n",
    "1. DataFrame total\n",
    "2. column total\n",
    "3. row total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e1e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_to_portions(df, variant = 'total'):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    - df: DataFrame containing counts to turn into portions\n",
    "    - variant: choose between 'total', 'col', or 'row' for how to calculate\n",
    "    portions\n",
    "    \n",
    "    OUTPUT:\n",
    "    - portion_df: returns a DataFrame of decimals depending on variant chosen\n",
    "    \"\"\"\n",
    "    \n",
    "    if variant == 'total': \n",
    "        # total portions\n",
    "        total = df.sum().sum()\n",
    "        portion_df = df / total\n",
    "    elif variant == 'col':\n",
    "        # column portions\n",
    "        portion_df = df.copy()\n",
    "        for col in df:\n",
    "            portion_df[col] = df[col] / df[col].sum()\n",
    "    else:\n",
    "        # row portions\n",
    "        portion_df = df.copy()\n",
    "        for row in df.index:\n",
    "            portion_df.loc[row] = df.loc[row] / df.loc[row].sum()\n",
    "    \n",
    "    return portion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46181b87",
   "metadata": {},
   "source": [
    "## Function 5: expand_categories(df, df_encoder, pred_value, prediction_input)\n",
    "Given a single datapoint to plug into a model which has been encoded (i.e. each categorical value has been encoded by the binary method), this function will expand the categorical data from the input to match the input required for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0ec62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_categories(df, df_encoder, pred_value, prediction_input):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    - df: DataFrame containing all of the data, not yet encoded or split\n",
    "    - df_encoder: DataFrame created after running through OneHotEncoder (must\n",
    "    have columns specified through encoder.get_feature_names_out())\n",
    "    - pred_value: Value to model is set up to predict\n",
    "    - prediction_input: list with all of the fields of a row in df (can put\n",
    "    dummy variable in for pred_value as it will be dropped)\n",
    "    \n",
    "    OUTPUT:\n",
    "    - full_input: array with original input transformed to mimic a row in the\n",
    "    predictor matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # predicting values\n",
    "    # prediction_input = list(df_subset.iloc[0].drop('Priority'))\n",
    "    \n",
    "    removable = df.columns.get_loc(pred_value)\n",
    "    del prediction_input[removable]\n",
    "    set_cols = df.columns.drop(pred_value)\n",
    "    encoder_cols = list(df_encoder.columns)\n",
    "    \n",
    "    input_expanded = np.zeros(df_encoder.shape[1])\n",
    "    input_concise = []\n",
    "    for count, pred_value in enumerate(prediction_input):\n",
    "        if type(pred_value) == str:\n",
    "            expanded_label = set_cols[count]+'_'+pred_value\n",
    "            input_expanded[encoder_cols.index(expanded_label)] = 1\n",
    "        else:\n",
    "            input_concise.append(pred_value)\n",
    "    \n",
    "    input_concise = np.array(input_concise)\n",
    "    full_input = np.concatenate((input_concise,input_expanded))\n",
    "    \n",
    "    return full_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
